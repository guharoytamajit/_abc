Role based access control:
Very important beacuse all cluster user should not have same permissions.

In RBAC we define Role and ClusterRule

Role- set of rules
Rule- contains a set of verbs(action),which we can do in a set of resources,and a set of apigroups("" indicate core api group)

Role are bound to a namespace,but ClusterRole is cluster-wide(all namespaces).


apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods","services"]    #["*"] would select all resources
  verbs: ["get", "watch", "list"]


---------------
- apiGroups: [""] 
  resources: ["*"]

This configuration will grant access to pods,services but not deployment,because of  apiGroups.

example  of api version:
services => v1(core api group)
pods => v1(core api group)
deployment =>  apps/v1(not part of core api)

To access deployment we need to define following Role:

- apiGroups: ["","apps"]  #access to apps and core api group  
  resources: ["*"]

Note: in api groups also we can define "*" to give access to all apiGroups (including core api)

----------------
We have already defined Role now time to define users.
K8s does not have an object that represents user
In k8s we  define an k8s object of type RoleBinding which builds Subjects(user list) to a Role.

For kubernetes user is just a string,we need to provide our own authentication system
======================

>kubectl config view //see configuration we are using when using kubectl commands
 shows details like list of cluster,list of contexts,list of users,current-context.
 context links a cluster to a user
 this is the same content inside ~/.kube/config
 
Adding new cluster for a now user,login as user
>kubectl config set-cluster <cluster-name> --server=<kubernetMaster>:<port>
>kubectl config view #now new cluster will be shown

set context:
>kubectl config set-context <context-name> --user <username> --cluster <clustername> 
>kubectl config view #now new context will be shown

update/set current-context:
>kubectl config use-context  <any-existing-context-name>

--------------------
k8s cluster has a Certificate Authority(CA) ie. a pair of secret file
From remote machine kubectl command will only work is the user has a certificate signed by k8s CA

Generate a key-pair:
>openssl genrsa -out my-private-key.key 2048   //create an rsa key pair of size 2048
coy this to the users home directory and change its permission,so other user cannot see this.

create a certificate signing request (CSR):
>openssl req -new -key my-private-key.key -out req.csr -subj "/CN=<user-name>/O=<user-name(actually groupname)>"

CA files are located in /etc/kubernetes/pki(ca.key,ca.crt) 
generate user gertiface from usert CSR using CA private-key() and CA certificate
>openssl x509 -req  in req.csr -CA ca.key -CAkey ca.key -CAcreateserial -out my-certificate.crt -days 365


install certificate for user(note it already has private key for that):
go to user home directory ,create ~/.certs directory.
here place user privatekey,user crt,and ca.crt these three files.(ca.key is absolute private to ks8)
Make the user the owner of this dirctory 
>sudo chowm -R newuser:newuser /home.newuser/.certs/

>su - newuser // "-" option will also load environment of the new user

add user in .kube/config
>kubectl config  set-credentials <any-name(generally username)> --client-certificate=<new user crt relative path from pwd> --client-key=<new user key relative path from pwd>
>kubectl config view //check changes
We also need to set CA certificate for everything to work:
>kubectl config set-cluster <cluster-name> --certificate-authority=<path of ca.crt with respect to pwd>

Now new user will be able to do "kubectl get all" provided proper RBAC is set.
 










