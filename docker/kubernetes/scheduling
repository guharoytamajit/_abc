NodeSelector:
It allows us to add a constraint about running a pod in a specific worker node.

Use Case:
Run appA in a node which has SSD
Deploy app in one of the node which is tagged for production(with better hardware).

to node you can add labels like:
disktype:hdd  or disktype:ssd
prod:true or prod:false


Add label to node:
>kubectl label node node1 disktype=ssd
>kubectl label node node2 disktype=hdd

Launch a pod with nodeSelector(disktype: ssd):

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd

It should be scheduled in node2
================================================
Node affinity:
Run a pod on a specific worker node
Node affinity is a set of rules used by the scheduler to determine where a pod can be placed.
NodeAffinity is little more flexible than NodeSelector

1)requiredDuringSchedulingIgnoredDuringExecution (Hard Preference):Constraint must match
2)preferredDuringSchedulingIgnoredDuringExecution (Soft Preference): even if the constraints in not fulfilled sechedular can still place pod in a  node.

Hard and Soft:
Sometimes, it’s ok to schedule a Pod on a Node that doesn’t have the labels you want. That’s why there are two different ways to use the selectors we defined earlier.
Hard Rule:
This Pod needs an amd64 Node: beta.k8s.io/arch=amd64
If no amd64 Nodes are available, the Pod cannot run(pending state).
Soft Rule:
This Pod prefers m4-large Nodes: beta.k8s.io/instance-type=m4-large:soft
Annotating the rule with “:soft” indicates that it is only a preference, not a requirement.
If an m4-large Node is available, the Pod will run there.
If no m4-large Nodes are available, the Pod can still be scheduled elsewhere.


kubectl label nodes <your-node-name> loadType=user

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: loadType
            operator: In
            values:
            - user            
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "loadType"
    value: user
    operator: "Equal"
    effect: "NoSchedule"

====================================
pod affinity and pod anti affinity:

Node Affinity ensures that pods are hosted on particular nodes.

Pod Affinity ensures two pods to be co-located in a single node.
usecase:
what if I want to run a BackEnd pod in the same node(or Az, or Region)  as AppA pod?    -use pod affinity
Or what if I want to run a BackEnd pod in a different node(or Az, or Region) where AppA pod is running?   -use  pod anti affinity


=====================================
Static pod(scheduling a pod without schedular):
You can directly inform kuelet to start a pod
pods created directly without schedulers are called static pods.
create a yaml in /etc/kubernetes/manifest
just create the file we dont have to apply it,this path is scanned after regular interval
similarly just delete the file from this location to un-apply

check location:
systemctl status kubelet
cd /etc/systemd/system/kubelet.service.d


=========================================
Taints and Toleration:

Taints: are used to repel scheduling of the pods from a specific nodes.

Toleration:
In order to enter the taint worker node, you need a special pass.
This pass is called toleration.

 
Apply taint to a node:
>kubectl taint nodes node1 key=value:NoSchedule

Now if you schedule any pod in the cluster,none of them will be scheduled in node1 because of the taint.
If we configure toleration in in deployment then only those pods will be allowed to schedule on tainted node(it doesnot mean all pods will be scheduled in the same node only repultion will be cancelled)

example:



Cancel taint to a node:
>kubectl taint nodes node1 key=value:NoSchedule-    #  "-" at the end will un-taint the node

we can view if a taint is present or not by >kubectl describe node node1.

refer taint_toleration.jpg to see possible values when configuring taint.

Taint Effects:
NoSchedule: New pod that donot match the taint are not scheduled into the node.Existing pods on the node remains.
PreferNoSchedule:New pod that do not match the taint might be scheduled in the node ,but scheduler tries not to.Existing pods on the node remains.
NoExecute:New pod that donot match the taint are not scheduled into the node. Existing pods on node that donot have a matching toleration are removed.

Taint Operators:
Equal: key-value-effect must match,this is default operator if not provided.
Exists:key-effect must match(value not considered),you must leave a blank value parameter which matches any.




