The metrics  falls into three broad categories:
1)Cluster state metrics
2)Resource metrics from Kubernetes nodes and pods
3)Work metrics from the Kubernetes Control Plane
--------------------------------
Where Kubernetes metrics come from?
The Kubernetes ecosystem includes two complementary add-ons for aggregating and reporting valuable monitoring data from your cluster: Metrics Server and kube-state-metrics.

1)Metrics Server collects resource usage statistics from the kubelet on each node and provides aggregated metrics through the Metrics API. 
2)kube-state-metrics: Whereas Metrics Server exposes metrics on resource usage by pods and nodes, kube-state-metrics listens to the Control Plane API server for data on the overall status of Kubernetes objects (nodes, pods, Deployments, etc.), as well as the resource limits and allocations for those objects. It then generates metrics from that data that are available through the Metrics API.
--------------------------------

Key Kubernetes performance metrics to monitor:
1)Cluster state metrics:
 The Kubernetes API server emits data about the count, health, and availability of various Kubernetes objects, such as pods.

 Retrieving cluster state metrics:
    Some cluster state statistics are automatically available from your cluster and can be retrieved using the kubectl command line utility. For example, you can use kubectl to retrieve a list    
    of Kubernetes Deployments and their current status:   > kubectl get deployments
  
    The kube-state-metrics add-on makes this data more accessible and consumable, especially for long-term monitoring.
    https://github.com/kubernetes/kube-state-metrics/tree/master/docs

	Metric: Node status
	Name in kube-state-metrics: kube_node_status_condition  (https://github.com/kubernetes/kube-state-metrics/blob/master/docs/node-metrics.md)
	Description: Current health status of the node. Returns a set of node conditions (listed below) and true, false, or unknown for each
	Metric type: Resource: Availability
        Metrics to alert :
		It runs checks on the following node conditions:

		a)OutOfDisk
		b)Ready (node is ready to accept pods)        => kube_node_status_condition{condition="Ready",status="true"}!=1
		c)MemoryPressure (node memory is too low)     =>kube_node_status_condition{condition="MemoryPressure",status="true"}!=1
		d)PIDPressure (too many running processes)    =>kube_node_status_condition{condition="PIDPressure",status="true"}!=1
		e)DiskPressure (remaining disk capacity is too low)  =>kube_node_status_condition{condition="DiskPressure",status="true"}!=1
		f)NetworkUnavailable
              Each check returns true, false, or, if the worker node hasn’t communicated with the relevant Control Plane node for a grace period (which defaults to 40 seconds), unknown. 




	Metric: Desired  pods
	Name in kube-state-metrics: kube_deployment_spec_replicas(desired pods )  or kube_daemonset_status_desired_number_scheduled
	Description: Number of pods specified for a Deployment or DaemonSet
	Metric type: Other
        Metrics to alert:
                      



	Metric: Current(Actual) pods
	Name in kube-state-metrics: kube_deployment_status_replicas(Actual pod count(still may not be ready)) or kube_daemonset_status_current_number_scheduled
	Description: Number of pods currently running in a Deployment or DaemonSet
	Metric type:  Other
        Metrics to alert:
                     if kube_deployment_spec_replicas != kube_deployment_status_replicas  for a long period
                     if kube_daemonset_status_desired_number_scheduled != kube_daemonset_status_current_number_scheduled  for a long period



	Metric: Available pods
	Name in kube-state-metrics: kube_deployment_status_replicas_available or kube_daemonset_status_number_available
	Description: Number of pods currently available for a Deployment or DaemonSet(A pod may be running but not available, meaning it is not ready and able to accept traffic.)
                      if you see spikes in the number of unavailable pods, or pods that are consistently unavailable, it might indicate a problem with their configuration.
	Metric type: Resource: Availability
        Metrics to alert:
                    if kube_deployment_spec_replicas != 0  and kube_deployment_status_replicas_available == 0  for a long time



	Metric: Unavailable pods
	Name in kube-state-metrics: kube_deployment_status_replicas_unavailable or kube_daemonset_status_number_unavailable
	Description: Number of pods currently not available for a Deployment or DaemonSet,In particular, a large number of unavailable pods might point to poorly configured readiness probes
	Metric type: Resource: Availability
        Metrics to alert:
               if you see spikes in the number of unavailable pods, or pods that are consistently unavailable, it might indicate a problem




2)Resource Metrics:
In a manifest, you can declare a request and a limit for CPU (measured in cores) and memory (measured in bytes) for each container running on a pod.
Requests & limits do not define a pod’s actual resource utilization,but they significantly affect how Kubernetes schedules pods on nodes.New pods will only be placed on a node that can meet their requests.

Comparing resource utilization with resource requests and limits will provide a more complete picture of whether your cluster has the capacity to run its workloads and accommodate new ones. 






	Metric: Memory requests
	Name in kube-state-metrics: kube_pod_container_resource_requests_memory_bytes
	Description: Total memory requests (bytes) of a pod
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: Memory limits
	Name in kube-state-metrics: kube_pod_container_resource_limits_memory_bytes
	Description: Total memory limits (bytes) of a pod
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: Allocatable memory
	Name in kube-state-metrics: kube_node_status_allocatable_memory_bytes
	Description: Total allocatable memory (bytes) of the node
	Metric type: Resource: Utilization
        Metrics to alert:



	Metric: Memory utilization
	Name in kube-state-metrics: N/A (see next section how to do this)
	Description: Total memory in use on a node or pod
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: CPU requests
	Name in kube-state-metrics: kube_pod_container_resource_requests_cpu_cores
	Description: Total CPU requests (cores) of a pod
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: CPU limits
	Name in kube-state-metrics: kube_pod_container_resource_limits_cpu_cores
	Description: Total CPU limits (cores) of a pod	
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: Allocatable CPU
	Name in kube-state-metrics: kube_node_status_allocatable_cpu_cores
	Description: Total allocatable CPU (cores) of the node
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: CPU utilization
	Name in kube-state-metrics: N/A
	Description: Total CPU in use on a node or pod
	Metric type: Resource: Utilization
        Metrics to alert:




	Metric: Disk utilization
	Name in kube-state-metrics: N/A
	Description: Total disk space used on a node
	Metric type: Resource: Utilization
        Metrics to alert:
----------------------------------------
important resource : https://github.com/wuestkamp/k8s-example-resource-monitoring

CPU Metrics

We use the following Prometheus queries:

# metrics are for k8s till 1.15
# for >=1.16 use pod instead of pod_name and container instead container_name
# container usage
rate(container_cpu_usage_seconds_total{pod=~"compute-.*", image!="", container_name!="POD"}[5m])

# container requests
avg(kube_pod_container_resource_requests_cpu_cores{pod=~"compute-.*"})

# container limits
avg(kube_pod_container_resource_limits_cpu_cores{pod=~"compute-.*"})

# throttling
rate(container_cpu_cfs_throttled_seconds_total{pod=~"compute-.*", container_name!="POD", image!=""}[5m])

Regarding the units (more):

    500m = 500millicore = 0.5 core
    500m = 500 millicpu = 0.5 cpu
----------------------------------------
Memory Metrics

We use the following Prometheus queries:

# metrics are for k8s till 1.15
# for >=1.16 use pod instead of pod_name and container instead container_name
# container usage
container_memory_working_set_bytes{pod_name=~"compute-.*", image!="", container_name!="POD"}

# container requests
avg(kube_pod_container_resource_requests_memory_bytes{pod=~"compute-.*"})

# container limits
avg(kube_pod_container_resource_limits_memory_bytes{pod=~"compute-.*"})

----------------------------------------
Testing with load:

The application consists of a simple deployment using the image gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5. 
It provides an HTTP endpoint and can receive commands to use resources:

>curl --data "millicores=400&durationSec=600" 10.12.0.11:8080/ConsumeCPU

>curl --data "megabytes=300&durationSec=600" 10.12.0.11:8080/ConsumeMem
------------------------------------



3)Control Plane metrics
The Kubernetes Control Plane provides several core services and resources for cluster management:
API server ,controller managers,scheduler,etcd
Kubernetes exposes metrics for each of these components, which you can collect and track to ensure that your cluster’s central nervous system is healthy. 

Note that in managed Kubernetes environments (such as Google Kubernetes Engine or Amazon Elastic Kubernetes Service clusters), the Control Plane is managed by the cloud provider, and you may not have access to all the components and metrics listed below. Also, the availability or names of certain metrics may be different depending on which version of Kubernetes you are using.


	Metric: etcd_server_has_leader
	Description: Indicates whether the member of the cluster has a leader (1 if a leader exists, 0 if not)
	Metric type: Resource: Availability
        Metrics to alert: Except during leader election events, the etcd cluster should always have a leader, which is necessary for the operation of the key-value store. If a particular member of an 
                     etcd cluster reports a value of 0 for etcd_server_has_leader (perhaps due to network issues), that member of the cluster does not recognize a leader and is unable to serve queries.
                     Therefore, if every cluster member reports a value of 0, the entire etcd cluster is down,this prevents Kubernetes from making any changes to cluster state. 


	Metric: etcd_server_leader_changes_seen_total
	Description: Counter of leader changes seen by the cluster member
	Metric type: other
        Metrics to alert:


	Metric: apiserver_request_latencies_count
	Description: Count of requests to the API server for a specific resource and verb
	Metric type: Resource: Work: Throughput
        Metrics to alert:


	Metric: apiserver_request_latencies_sum
	Description: Sum of request duration to the API server for a specific resource and verb, in microseconds
	Metric type: Work: Performance
        Metrics to alert:


	Metric: workqueue_queue_duration_seconds (v1.14+)
	Description: Total number of seconds that items spent waiting in a specific work queue
	Metric type: Work: Performance
        Metrics to alert:


	Metric: workqueue_work_duration_seconds (v 1.14+)
	Description: Total number of seconds spent processing items in a specific work queue
	Metric type: Work: Performance
        Metrics to alert:


	Metric: scheduler_schedule_attempts_total
	Description: Count of attempts to schedule a pod, broken out by result
	Metric type: Work: Throughput
        Metrics to alert:


	Metric: scheduler_e2e_scheduling_latency_microseconds (pre-v1.14) or scheduler_e2e_scheduling_duration_seconds (v1.14+)
	Description: Total elapsed latency in scheduling workload pods on worker nodes
	Metric type: Work: Throughput
        Metrics to alert:







 




