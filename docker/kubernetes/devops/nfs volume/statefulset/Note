>mkdir -p /mnt/sharedfolder/{pv0,pv1,pv2,pv3,pv4}
>kubectl apply -f sts-pv.yaml
>kubectl apply -f sts-nginx.yaml


>kubectl get pv,pvc          #we can see only two pv are bound as replicas=2,other pv are available for use                    
NAME                          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                  STORAGECLASS   REASON   AGE
persistentvolume/pv-nfs-pv0   200Mi      RWO            Retain           Bound       demo/www-nginx-sts-0   manual                  12m
persistentvolume/pv-nfs-pv1   200Mi      RWO            Retain           Bound       demo/www-nginx-sts-1   manual                  12m
persistentvolume/pv-nfs-pv2   200Mi      RWO            Retain           Available                          manual                  12m
persistentvolume/pv-nfs-pv3   200Mi      RWO            Retain           Available                          manual                  12m
persistentvolume/pv-nfs-pv4   200Mi      RWO            Retain           Available                          manual                  12m

NAME                                    STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/www-nginx-sts-0   Bound    pv-nfs-pv0   200Mi      RWO            manual         12m
persistentvolumeclaim/www-nginx-sts-1   Bound    pv-nfs-pv1   200Mi      RWO            manual         98s

>kubectl get statefulset,po,svc,endpoints -o wide
NAME                         READY   AGE   CONTAINERS   IMAGES
statefulset.apps/nginx-sts   2/2     17m   nginx        nginx

NAME              READY   STATUS    RESTARTS   AGE   LABELS
pod/nginx-sts-0   1/1     Running   0          58m   controller-revision-hash=nginx-sts-748657747c,run=nginx-sts-demo,statefulset.kubernetes.io/pod-name=nginx-sts-0
pod/nginx-sts-1   1/1     Running   0          48m   controller-revision-hash=nginx-sts-748657747c,run=nginx-sts-demo,statefulset.kubernetes.io/pod-name=nginx-sts-1



NAME                     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/nginx-headless   ClusterIP   None         <none>        80/TCP    17m   run=nginx-sts-demo

NAME                       ENDPOINTS                       AGE
endpoints/nginx-headless   10.244.1.64:80,10.244.2.66:80   17m


where dns => ip is:
nginx-sts-0.nginx-headless.demo.svc.cluster.local =>10.244.2.66
nginx-sts-1.nginx-headless.demo.svc.cluster.local =>10.244.1.64
<StatefulSet(nginx-sts)>-<Ordinal>.<Service(nginx-headless)>.<Namespace(demo)>.svc.cluster.local


Here ping to headless service are not load balanced but has the following behaviour:
From nginx-sts-0 pod
>ping nginx-headless   #pings nginx-sts-0.nginx-headless.demo.svc.cluster.local

From nginx-sts-1 pod
>ping nginx-headless   #pings nginx-sts-1.nginx-headless.demo.svc.cluster.local

From another pod outside statefulset,it will load balance
>ping nginx-headless  #sometimes ping nginx-sts-0.nginx-headless.demo.svc.cluster.local,sometime pings nginx-sts-0.nginx-headless.demo.svc.cluster.local

----------------------------------------
If headless service does not load balancee them from outside cluster?
How do you properly expose a StatefulSet externally?
Ans:  The created headless service will not be given a clusterIP, but will instead simply include a list of Endpoints. These Endpoints are then used to generate instance-specific DNS records in the form of: <StatefulSet>-<Ordinal>.<Service>.<Namespace>.svc.cluster.local e.g., app-0.myapp.default.svc.cluster.local.

Then how to loadbalance across stateful pods for non-cluster requests?
Ans: The Kubernetes developers have thought of this already. ;) Each Pod in the StatefulSet has itâ€™s own label that includes its generated Pod identity: statefulset.kubernetes.io/pod-name. You know what that means? It means we can use it in a Service Selector!
While the headless service might not take care of all our needs, we can create additional Services that point to the individual Pods of the StatefulSet. 

apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  type: LoadBalancer
  selector:
    run: nginx-sts-demo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
==============
cleanup:
kill all pods,svc,statefulset:
>kubectl scale sts nginx-sts --replicas=0
>kubectl delete all --all
kill all pvc:
>kubectl delete pvc --all

used pv will now be in released state but still not available for use:
>kubectl get pv    
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                  STORAGECLASS   REASON   AGE
pv-nfs-pv0   200Mi      RWO            Retain           Released    demo/www-nginx-sts-0   manual                  82m
pv-nfs-pv1   200Mi      RWO            Retain           Released    demo/www-nginx-sts-1   manual                  82m
pv-nfs-pv2   200Mi      RWO            Retain           Available                          manual                  82m
pv-nfs-pv3   200Mi      RWO            Retain           Available                          manual                  82m
pv-nfs-pv4   200Mi      RWO            Retain           Available                          manual                  82m


so we have to delete pv also:
>kubectl delete pv --all
==================================================
PersistentVolumes can have various reclaim policies, including "Retain", "Recycle", and "Delete". 
For dynamically provisioned PersistentVolumes, the default reclaim policy is "Delete". 
 This means that a dynamically provisioned volume is automatically deleted when a user deletes the corresponding PersistentVolumeClaim. 
 This automatic behavior might be inappropriate if the volume contains precious data. In that case, it is more appropriate to use the "Retain" policy. 
 With the "Retain" policy, if a user deletes a PersistentVolumeClaim, the corresponding PersistentVolume is not be deleted. Instead, it is moved to the Released phase, where all of its data can be manually recovered
 
 
 >kubectl get pv
 

 change reclaim policy 
 >kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
 
 Verify
  >kubectl get pv
  ==================================================
  Deleting Statefulset:
  
  You can delete a StatefulSet in the same way you delete other resources in Kubernetes: use the kubectl delete command, and specify the StatefulSet either by file or by name.
  >kubectl delete statefulsets <statefulset-name>
  
  You may need to delete the associated headless service separately after the StatefulSet itself is deleted.
  >kubectl delete service <service-name>
  
 Deleting a StatefulSet through kubectl will scale it down to 0, thereby deleting all pods that are a part of it. 
 If you want to delete just the StatefulSet and not the pods, use --cascade=false.
>kubectl delete -f <file.yaml> --cascade=false
  
  Deleting the Pods in a StatefulSet will not delete the associated volumes. This is to ensure that you have the chance to copy data off the volume before deleting it. 
  Deleting the PVC might trigger deletion of the backing PV depending on the storage class and reclaim policy.
  
  
  Delete  sts together with PVC:
 >grace=$(kubectl get pods <stateful-set-pod> --template '{{.spec.terminationGracePeriodSeconds}}')
>kubectl delete statefulset -l app=myapp
>sleep $grace
>kubectl delete pvc -l app=myapp






