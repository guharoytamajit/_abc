Reference: http://gokhanatil.com/2018/02/build-a-cassandra-cluster-on-docker.html



<<<<<<< HEAD
>docker run --name cas1 -p 9042:9042 -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d  -v //c/Users/vol:/vol cassandra:3.11
>docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1

>docker run --name cas2 -e CASSANDRA_SEEDS="$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1)" -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d -v //c/Users/vol:/vol cassandra:3.11

>docker run --name cas3 -e CASSANDRA_SEEDS="$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1)" -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter2 -d -v //c/Users/vol:/vol cassandra:3.11
=======
>docker run --name cas1 -p 9042:9042 -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d cassandra:3.11
>docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1

>docker run --name cas2 -e CASSANDRA_SEEDS="$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1)" -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter1 -d cassandra:3.11

>docker run --name cas3 -e CASSANDRA_SEEDS="$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' cas1)" -e CASSANDRA_CLUSTER_NAME=MyCluster -e CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch -e CASSANDRA_DC=datacenter2 -d cassandra:3.11
>>>>>>> b9bbeee5d35f0fc57f4e39955c7b8e2ebe5dbae1

>docker exec -ti cas1 nodetool status

If your new nodes didn’t show up in the list, they probably crashed before they joined the cluster. In this case, you may restart the missing nodes. For example, 
if cas3 (the last node) didn’t joined to the cluster and it’s down, you can run “docker start cas3” command to start it. It’ll try to join the cluster automatically.

The status column of each node should show UN (node is UP and its state is Normal). If you see “UJ” that means your node is joining, just wait for a while and check it again. 

Now Let’s create a keyspace (database) which will be replicated to datacenter1 and datacenter2, and a table in this newly created keyspace. I’ll use NetworkTopologyStrategy for 
replicating data. Each datacenter will store one copy of data.

>docker exec -ti cas1 cqlsh

CREATE KEYSPACE mykeyspace WITH replication = {'class' : 'NetworkTopologyStrategy','datacenter1' : 1,'datacenter2' : 1};
<<<<<<< HEAD
CREATE TABLE mykeyspace.emp (id int,first_name text,last_name text,email text,gender text,salary int,primary key(id,first_name));
COPY mykeyspace.emp(id,first_name,last_name,email,gender,salary) FROM '/vol/emp.csv' WITH DELIMITER=',' AND HEADER=TRUE;

>docker exec -ti cas1 nodetool status mykeyspace


Did you notice that the nodes at datacenter1 shares data almost evenly, while the node at datacenter2 has replication of all data? Remember the replication strategy of our 
keyspace: Each datacenter stores one copy. Because there are two nodes in datacenter1, the data will be evenly distributed between these two nodes.

You can shutdown nodes using “docker stop cas1 cas2 cas3” and then start them again with “docker start cas1 cas2 cas3”. So, we have a working Cassandra cluster which is 
deployed to multiple data centers. Use it for your tests! Do not forget to check my presentation about Cassandra.


Note:
config file(eg. cassandra.yaml) location inside docker image:  /etc/cassandra/cassandra.yaml
binaries(nodetool, cassandra) are present inside: /usr/sbin/cassandra and /usr/bin/cassandra
=======
CREATE TABLE mykeyspace.mytable (id int primary key,name text);


>docker exec -ti cas1 nodetool status mykeyspace
>>>>>>> b9bbeee5d35f0fc57f4e39955c7b8e2ebe5dbae1

