kafka cluster have multiple nodes/brokers/servers
broker can have multiple topics
topic can have multiple partitions
kafka uses concept of partitioning for scaling
partitions are replicated to provide a fault taularent system
messages inside partition are ordered and can be identified by index
a message can be uniquely identified by combination of(topic,partition,index)
partitions of same topic are replicated across multiple nodes,but only one of them is leader for each partition
All reads and writes goes to leader partition and replicas tries to sync itself, producer/consumer never interacts with replicas.
For best scalibility different partition leader should be kept on different nodes
producer api by default distribute(load balance) messages across different partitions but also enables us to send a message to a particular partition
two or more consumers together can form a consumer group,each consumer in a consumer group will receive a part of data.
each consumer in consumer group gets unique messages(no two consumer can get the same message in same consumer group)
each consumer in consumer group can subscribe for one or more partition but not vice versa ie. 1 partition cannot be subscrribed by more than 1 consumer in same cg.
So there is no extra advantage if No. of consumer is > No. of partition(extra consumers will not do any work)
consumer group allows scaling
each consumer and consumer group as a whole will receive all messages of topic it has subscribed for.
messages in partitions are stored as log and not immeditely removed, so if consumer lost old messages  it can get them back by offset(index)
kafka depends on zookeeper to maintain state of cluster
======================
setup 2 node cluster:

1.goto kafka/config/server.properties
 broker.id=<id> field should be unique for all nodes in the same cluster.we can use 0,1,2.. so on.
 port=<number> the port server listens to,keep this different if you are running multiple broker in same machine to avoid conflict
 log.dir=<path to dir> keep different path  if you are running multiple broker in same machine to avoid confusion
 num.partitions=<num>
2.start zookeeper:
>bin/zookeeper-server-start.sh conf/zookeeper.properties

3.start broker:
>bin/kafka-server-start.sh config/server.properties

4.Start another server:
copy server.properties and create server2.properties and change broker.id,port,log.dir
>bin/kafka-server-start.sh config/server2.properties

both server must point to the same zookeeper

5.create a topic:
>bin/kafka-topics.sh --zookeeper <host:port in server.properties> --create --topic topic1 --partitions 2 --replication-factor 2(<= No of servers)

6.View details of topic:
>bin/kafka-topics.sh --zookeeper <host:port> --describe --topic topic1

7.produce message:
>bin/kafka-console-producer.sh --broker-list localhsot:9092 --topic topic1

8.consume message:
>bin/kafka-console-consumer.sh --zookeeper <host:port> --topic topic1


Now if we type something in producer console,all those messages will be updated on consumer console.

Get all message from beginning:
>bin/kafka-console-consumer.sh --zookeeper <host:port> --topic topic1 --from-beginning

zookeeper-server-start.bat ../../config/zookeeper.properties
kafka-server-start.bat   ../../config/server.properties


kafka-topics.bat --zookeeper localhost:2181 --create --topic topic1 --partitions 1 --replication-factor 1
kafka-console-producer.bat --broker-list localhost:9092 --topic topic1
kafka-console-consumer.bat --zookeeper localhost:2181 --topic topic1

=====================================
Producer important properties:
1.bootstrap.servers => provides the initial hosts that act as the starting point for a Kafka client to discover the full set of alive servers in 
the cluster.Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), 
this list does not have to contain the full set of servers (you may want more than one, though, in case a server is down). eg "localhost:9091,localhost:9093"
2.key.serializer=>used to serailize message key(this must match with de-serializer used in consumer)
3.value.serializer=>used to serailize message value(this must match with de-serializer used in consumer)


Consumer properties:
1.bootstrap.servers
2.key.serializer
3.value.serializer
4.group.id=>name of consumer group
====================================
Tuning:
A)producer:
1.request.required.acks(how long to wait after sending message to broker)
  possible values:
  0=>producers never wait for acknowledgement(very low latency at the cost of duraility)
  1=>producer gets ack after leader replica has received
  -1=>producer gets ack after in-sync replica has received
