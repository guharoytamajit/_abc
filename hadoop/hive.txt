hive need not be installed in all machines in hadoop cluster.
hive is like a client to hadoop cluster, it converts sql to multiple mapreduce tasks and then submit the task in hadoop cluster.
hive is a datawarehouse built on top of hadoop
hive stores its data as files in HDFS,so it is automatically distributed and replicated.
Hive does not need additional process like datanaode or namenode or resource manager.
Hive's performance is improved by partitioning, where RDBMS's performance can be improved by indexing. 
it should instead be used for making aggregation queries.
Hive does not support row level update or delete. 
 
 
 hive metastore:
  All hive tables has metadata(columns ,types ,schema etc), these matadata is stored in metastore. 
  This matastore tells hive how to read a file from HDFS and convert it into row,column representation.
  If we have a csv file in HDFS, metastore will know how to parse the csv into rows and columns
  hive allows the user to configure metastore,we can use any traditional RDBMS technology like derby(default),mysql.
  so hive is not a relational database but with the help of matastore it gives  a feeling like that.
  
  
 Hive is only good for analytical queries and should not be used for transactional queries or realtime queries. 
 With hive we should make queries like summary or aggregation and should not pull individual records as it will not give any performance benefit.
   
RDBMS use schema on write,but hive uses schema on read(it enforces all schema constrains while reading).
There is no data check in write operations.
Since hive is unaware of schema during write it does not support row level update or delete. 
For same reason it also dont have support for primary key,foreign key,unique,not null.
In case of schema mismatch while reading hive tries its best to fix it by padding with null.
example:
NAME,AGE   <= Headers
alex
john
jim

In above record defination AGE is there but in actual record there in no age.
If you try to query age column it will return null.

Hive does not use schema on write because it is generally not the ownner of data, same data may be used by HBase and Cassandra as well.

Table Join:
Hive only supports equijoin ,does not support natural joins.

EquiJoin:
select a.firstname,b.lastname from employee a join subordinates b on a.empId=b.empId; //this is equijoin as we are using = operator in a.empId=b.empId

Natural Join:
natural joins are joins where join clause is implicit eg:
select a.firstname,b.lastname from employee a , subordinates b

Hive supports UNION but does not support INTERSECT and MINUS.

Hive has many more built-in functions,which are not available on SQL.It also enables us to create user define function.

==================
Install HIVE:
First install hadoop as defined in hadoop.txt under pseudo distributed mode section.

Inside ~/.bashrc:
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/usr/local/Hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.

source .bashrc // to reload .bashrc

set HADOOP_HOME in hive config directory inside hive-env.sh:
$ cd $HIVE_HOME/conf
$ cp hive-env.sh.template hive-env.sh
export HADOOP_HOME=/usr/local/hadoop
also set hive conf directory  //may not require need to check


hive-site.xml add:
<property>
   <name>javax.jdo.option.ConnectionURL</name>
   <value>jdbc:derby:;databaseName=metastore_db;create=true </value>              #if we dont want generation of metastore_db in current dir we can give absolute path over here instead eg. databaseName=/data/metastore_db
   <description>JDBC connect string for a JDBC metastore </description>
</property>
<!-- 
 <property>
   <name>hive.exec.local.scratchdir</name>
   <value>/tmp/hive_io</value>
   <description>Local scratch space for hive job</description>
</property>
-->

hadoop dfs –mkdir /tmp
hadoop dfs –mkdir –p /user/hive/warehouse
hadoop dfs –chmod g+w /tmp
hadoop dfs –chmod g+w /user/hive/warehouse

initialize metastore before you start using hive:
>schematool –initSchema –dbType derby   //delete metastore_db if already present in $HIVE_HOME/config
 
Now open hive shell and use hive:
>hive 
 

==============

>show tables;
>select * from student;
>describe student; //returns schema
>describe extende student; //detailed schema
>select count(*) from student;// this kind of aggregation query will trigger MR job

=========
Datatypes:
Primitive: BOOLEAN,Numeric,String,Timestamp
          NUMERIC=>TINYINT(-128 to 127),SMALLINT(-32768 to 32768),INT(-2^15 to 2^15-1),BIGINT(-2^63 to 2^63-1) 
		  DICIMALS =>FLOAT,DOUBLE,DECIMAL(<number length including decimal>,<decimal length>) eg DECIMAL(10,2)
		  STRING(variable length),CHAR(fixed size),VARCHAR(variable length upto a max size).  //CHAR(4) will always consume 4 spaces even is string has length 2, where as VHARCHAR(4) will not consume more space that length of actual string.
		  by default STRING is mapped to VARCHAR(32762)
          TIMESTAMP =>TIMESTAMP ,DATE
		  
Collection: Array,Struct,Map,Union
          ARRAY<VARCHAR(100)>  //array can hold only primitive types
          MAP<STRING,ARRAY<VARCHAR(100)>>		  
		  STUDENT{NAME(STRING),ID(INT),FEES(FLOAT)} eg. COL_A STRUCT<a:STRING,b:INT,c:BOOLEAN>
		  UNION  can have any datatype out of given set of datatypes eg COL_B  UNIONTYPE<TIMESTAMP,DATE,VARCHAR>
  
  
=========
Managed Tables  vs  External Tables:

Managed Tables:
Data is managed by hive and stored in its warehouse directory.

External Tables(data shared between multiple programs eg. cassandra,hbase,pig):
hive is not the owner of data and stored outside warehouse directory,but metadata still exists in hive metastore.
If table dropped only metastore content is removed,but main data remains there.
keyword 'external' is used to make a table external table.


CREATE DATABASE|SCHEMA [IF NOT EXISTS] <database name>

CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[ROW FORMAT row_format]
[STORED AS file_format]

example 1:

create table students
(
ID  INT,                   // cannot add NOT NULL,unique,auto_increment constrains
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
)
COMMENT 'Student details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED IN TEXT FILE;

example2:

create External table students
(
ID  INT,                   // cannot add NOT NULL,unique,auto_increment constrains
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
)
location '/user/student'; // in hdfs but not in warehouse


Temporary Tables(store temporary data):
Temporary tables are those tables which gets deleted at the end of the hive session.
Multiple users can create temporary table with same name as they reside in different sessions.
temporary tables doesnot support partitioned columns and indexes
If in same session there is a permanent and temporary table with same name, user cannot access permanent table without dropping temporary table.All references will refer temporary table.

example:
create temporary table students
(
ID  INT,                  
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
);


hive allows us to create a table by copying another table schema:
>create table new_table like old_table; 
===================
Load data in Hive:

1 Using standard  SQL insert into statement:
eg. 
insert into employee (eid, name, salary, destination) values (1, 'Franciska', 62123, 'Japan');

Insert if we follow the order defined in table;
insert into employee values (2, 'Wanids', 20921, 'Norway');

Multi record insert:
insert into employee values (3, 'Ilyssa', 45135, 'Brazil'),(4, 'Alphard', 63582, 'Colombia'),(5, 'Ethyl', 36972, 'Indonesia');
--- 
2.Import data directly to HDFS(warehouse dir):
>load data local inpath '/desktop/student_data.txt' <overwrite> into table student;
it will create  /user/hive/warehouse/student/student_data.txt in HDFS

overwrite  =>content of the target table will be replaced by content of file.Otherwise append by default.

multiple files can be loaded into same table,hive will consider the union of all those records during query.
>load data local inpath '/desktop/student_data2.txt'  into table student;
it will create  /user/hive/warehouse/student/student_data2.txt in HDFS

hive will expect these data files to be in a certain format defined in table schema like "field terminated by","rows terminated by" etc.

We can change the format of data for a table by alter table  command as:
>alter table student set serdeproperties('field.delim'=',');
serdeproperties (SERialization DEserialization properties)=> is a dictionary that tells hive how to read the files in table directory
---
3. Import data from another table:
we can create a table student_name which contains ID and FirstName of student table.

create table student_firstname(
ID  INT,                  
FirstName VRACHAR(100)
)

insert overwrite  table student_firstname select ID,FirstName from student;

Multi-table insert(in single scan):
We can insert data into two different tables from one source table in one command.

Lets create two table student_firstname and student_name which will be populated from student table.
  
create table student_firstname(                
FirstName VRACHAR(100)
)

create table student_name(
ID  INT,                  
FirstName VRACHAR(100),
LastName VRACHAR(100),
)

Following query will populate student_firstname and student_name:

from student
insert overwrite table student_firstname
select distinct FirstName                  
insert overwrite table student_firstname
select ID,FirstName,LastName;
========================
Alter Table:

Rename table:
>alter table old_name new_name;
OR
>alter table old_name rename to new_name;

Add column:
>alter table student add columns(col_a INT);
 
Delete column:
>alter table student replace columns (col_a INT,col_b DATE) //the columns which are not listed will be deleted.
 
NOTE:We only have to list those columns which we want to retain."replace" is actually  "retains"

Delete table:
>drop table student;

Delete table data:
>truncate table student;

Hive does not allow to update or delete.
=============================

Metadata:
The hive metastore service stores the metadata for hive tables and partitions.
This data is stored in a relational database and provides clients(including hive) access to this information via the metastore service API.
Information stored in metastore:
database ids,table ids,index ids, index creation time,table creation time. 

Hive default warehouse(data) directory is /user/hive/warehouse ut can be changed by updating property hive.metastore.warehouse.dir in hive xml config.

when we create a table using create table command, a directory with that name is created inside warehouse directory.
If databsse is not specified table directory is created directly inside warehouse directory.

hive table may be composed of multiple file(union of all file content is the content of table).
============
till 20
============
Hive Index:
In RDBMS:
  constraints=> helps in maintaining integrity
  indices => increase query speed

index=> index is like a fast lookup from a value to the row numbers in the table that contains that value.

hive allows index creation in two ways:
 1. build-in index
 2. using an index handler class
 
 TODO
 
 ===========
 Hive partitions:
  splitting up of data in table.
  each partition is a separate directory.
  a directory can have multiple data file in that partition.
  all partitions together make up the entire table.
  the partition can be done based on a function applied on  the value of one or more column. 
    
  


  
