hive need not be installed in all machines in hadoop cluster.
hive is like a client to hadoop cluster, it converts sql to multiple mapreduce tasks and then submit the task in hadoop cluster.
hive is a datawarehouse built on top of hadoop
hive stores its data as files in HDFS,so it is automatically distributed and replicated.
Hive does not need additional process(daemon) like datanaode or namenode or resource manager.
Hive's performance is improved by partitioning, where RDBMS's performance can be improved by indexing. 
it should instead be used for making aggregation queries.
Hive does not support row level update or delete. 
 
 
 hive metastore:
  All hive tables has metadata(columns ,types ,schema etc), these matadata is stored in metastore. 
  This matastore tells hive how to read a file from HDFS and convert it into row,column representation.
  If we have a csv file in HDFS, metastore will know how to parse the csv into rows and columns
  hive allows the user to configure metastore,we can use any traditional RDBMS technology like derby(default),mysql.
  so hive is not a relational database but with the help of matastore it gives  a feeling like that.
  
  
 Hive is only good for analytical queries and should not be used for transactional queries or realtime queries. 
 With hive we should make queries like summary or aggregation and should not pull individual records as it will not give any performance benefit.
   
RDBMS use schema on write,but hive uses schema on read(it enforces all schema constrains while reading).
There is no data check in write operations. 
Since hive is unaware of schema during write it does not support row level update or delete. 
For same reason it also dont have support for primary key,foreign key,unique,not null.
In case of schema mismatch while reading hive tries its best to fix it by padding with null.
example:
NAME,AGE   <= Headers
alex
john
jim

In above record defination AGE is there but in actual record there in no age.
If you try to query age column it will return null.

Hive does not use schema on write because it is generally not the owner of data, same data may be used by HBase and Cassandra as well.

Table Join:
Hive only supports equijoin ,does not support natural joins.

EquiJoin:
select a.firstname,b.lastname from employee a join subordinates b on a.empId=b.empId; //this is equijoin as we are using = operator in a.empId=b.empId

Natural Join:
natural joins are joins where join clause is implicit eg:
select a.firstname,b.lastname from employee a , subordinates b

Hive supports UNION but does not support INTERSECT and MINUS.

Hive has many more built-in functions,which are not available on SQL.It also enables us to create user define function.

==================
Install HIVE:
First install hadoop as defined in hadoop.txt under pseudo distributed mode section.

Inside ~/.bashrc:
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/usr/local/Hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/usr/local/hive/lib/*:.

source .bashrc // to reload .bashrc

set HADOOP_HOME in hive config directory inside hive-env.sh:
$ cd $HIVE_HOME/conf
$ cp hive-env.sh.template hive-env.sh
export HADOOP_HOME=/usr/local/hadoop
also set hive conf directory  //may not require need to check


hive-site.xml add:
<property>
   <name>javax.jdo.option.ConnectionURL</name>
   <value>jdbc:derby:;databaseName=metastore_db;create=true </value>              #if we dont want generation of metastore_db in current dir we can give absolute path over here instead eg. databaseName=/data/metastore_db
   <description>JDBC connect string for a JDBC metastore </description>
</property>
<!-- 
 <property>
   <name>hive.exec.local.scratchdir</name>
   <value>/tmp/hive_io</value>
   <description>Local scratch space for hive job</description>
</property>
-->

hadoop dfs –mkdir /tmp
hadoop dfs –mkdir –p /user/hive/warehouse
hadoop dfs –chmod g+w /tmp
hadoop dfs –chmod g+w /user/hive/warehouse

initialize metastore before you start using hive:
>schematool –initSchema –dbType derby   //delete metastore_db if already present in $HIVE_HOME/config
 
Now open hive shell and use hive:
>hive 
 

==============

>show tables;
>select * from student;
>describe student; //returns schema
>describe extende student; //detailed schema
>select count(*) from student;// this kind of aggregation query will trigger MR job

=========
Datatypes:
Primitive: BOOLEAN,Numeric,String,Timestamp
          NUMERIC=>TINYINT(-128 to 127),SMALLINT(-32768 to 32768),INT(-2^15 to 2^15-1),BIGINT(-2^63 to 2^63-1) 
		  DICIMALS =>FLOAT,DOUBLE,DECIMAL(<number length including decimal>,<decimal length>) eg DECIMAL(10,2)
		  STRING(variable length),CHAR(fixed size),VARCHAR(variable length upto a max size).  //CHAR(4) will always consume 4 spaces even is string has length 2, where as VHARCHAR(4) will not consume more space that length of actual string.
		  by default STRING is mapped to VARCHAR(32762)
          TIMESTAMP =>TIMESTAMP ,DATE
		  
Collection: Array,Struct,Map,Union
          ARRAY<VARCHAR(100)>  //array can hold only primitive types
          MAP<STRING,ARRAY<VARCHAR(100)>>		  
		  STUDENT{NAME(STRING),ID(INT),FEES(FLOAT)} eg. COL_A STRUCT<a:STRING,b:INT,c:BOOLEAN>
		  UNION  can have any datatype out of given set of datatypes eg COL_B  UNIONTYPE<TIMESTAMP,DATE,VARCHAR>
  
  
=========
Managed Tables  vs  External Tables:

Managed Tables:
Data is managed by hive and stored in its warehouse directory.

External Tables(data shared between multiple programs eg. cassandra,hbase,pig):
hive is not the owner of data and stored outside warehouse directory,but metadata still exists in hive metastore.
If table dropped only metastore content is removed,but main data remains there.
keyword 'external' is used to make a table external table.


CREATE DATABASE|SCHEMA [IF NOT EXISTS] <database name>

CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.] table_name
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[ROW FORMAT row_format]
[STORED AS file_format]

example 1:

create table students
(
ID  INT,                   // cannot add NOT NULL,unique,auto_increment constrains
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
)
COMMENT 'Student details'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n'
STORED IN TEXT FILE;

example2:

create External table students
(
ID  INT,                   // cannot add NOT NULL,unique,auto_increment constrains
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
)
location '/user/student'; // in hdfs but not in warehouse


Temporary Tables(store temporary data):
Temporary tables are those tables which gets deleted at the end of the hive session.
Multiple users can create temporary table with same name as they reside in different sessions.
temporary tables doesnot support partitioned columns and indexes
If in same session there is a permanent and temporary table with same name, user cannot access permanent table without dropping temporary table.All references will refer temporary table.

example:
create temporary table students
(
ID  INT,                  
FirstName VRACHAR(100),
LastName VRACHAR(100),
Gender CHAR(1),
Email VRACHAR(100),
);


hive allows us to create a table by copying another table schema:
>create table new_table like old_table; 
===================
Load data in Hive:

1 Using standard  SQL insert into statement:
eg. 
insert into employee (eid, name, salary, destination) values (1, 'Franciska', 62123, 'Japan');

Insert if we follow the order defined in table;
insert into employee values (2, 'Wanids', 20921, 'Norway');

Multi record insert:
insert into employee values (3, 'Ilyssa', 45135, 'Brazil'),(4, 'Alphard', 63582, 'Colombia'),(5, 'Ethyl', 36972, 'Indonesia');
--- 
2.Import data directly to HDFS(warehouse dir):
>load data local inpath '/desktop/student_data.txt' <overwrite> into table student;
it will create  /user/hive/warehouse/student/student_data.txt in HDFS

overwrite  =>content of the target table will be replaced by content of file.Otherwise append by default.

multiple files can be loaded into same table,hive will consider the union of all those records during query.
>load data local inpath '/desktop/student_data2.txt'  into table student;
it will create  /user/hive/warehouse/student/student_data2.txt in HDFS

hive will expect these data files to be in a certain format defined in table schema like "field terminated by","rows terminated by" etc.

We can change the format of data for a table by alter table  command as:
>alter table student set serdeproperties('field.delim'=',');
serdeproperties (SERialization DEserialization properties)=> is a dictionary that tells hive how to read the files in table directory
---
3. Import data from another table:
we can create a table student_name which contains ID and FirstName of student table.

create table student_firstname(
ID  INT,                  
FirstName VRACHAR(100)
)

insert overwrite  table student_firstname select ID,FirstName from student;

Multi-table insert(in single scan):
We can insert data into two different tables from one source table in one command.

Lets create two table student_firstname and student_name which will be populated from student table.
  
create table student_firstname(                
FirstName VRACHAR(100)
)

create table student_name(
ID  INT,                  
FirstName VRACHAR(100),
LastName VRACHAR(100),
)

Following query will populate student_firstname and student_name:

from student
insert overwrite table student_firstname
select distinct FirstName                  
insert overwrite table student_firstname
select ID,FirstName,LastName;
========================
Alter Table:

Rename table:
>alter table old_name new_name;
OR
>alter table old_name rename to new_name;

Add column:
>alter table student add columns(col_a INT);
 
Delete column:
>alter table student replace columns (col_a INT,col_b DATE) //the columns which are not listed will be deleted.
 
NOTE:We only have to list those columns which we want to retain."replace" is actually  "retains"

Delete table:
>drop table student;

Delete table data:
>truncate table student;

Hive does not allow to update or delete.
=============================

Metadata:
The hive metastore service stores the metadata for hive tables and partitions.
This data is stored in a relational database and provides clients(including hive) access to this information via the metastore service API.
Information stored in metastore:
database ids,table ids,index ids, index creation time,table creation time. 

Hive default warehouse(data) directory is /user/hive/warehouse ut can be changed by updating property hive.metastore.warehouse.dir in hive xml config.

when we create a table using create table command, a directory with that name is created inside warehouse directory.
If databsse is not specified table directory is created directly inside warehouse directory.

hive table may be composed of multiple file(union of all file content is the content of table).
============
When er fire insert into query a new MR job is launched.
insert into query may have one or multi row,in both cases a new file is created inside table directory(table content is union of all files).
we can see the content of each table files using >hadoop fs /user/hive/warehouse/{database}/{table}/{filename} //for external table location can be different

Note:
 in hive default field delimiter is not comma ','. so you may see many NULL when doint >select * from <table name>
 so for csv we need to do >alter table <tablename> set serdeproperties('field.delim'=',');
 Now >select * from <table name>;   will parse them correctly and give expected result.


Execute hive queries from outside hive shell(from system terminal):
>hive -e 'from student select firstname,email where gender ="M";'
syntax >hive -e '<query to be executed on hive shell>'

we can also pass a sql file which has multiple queries using -f option:
>hive -f abc.sql
===================
Hive Functions:
Built in functions of hive
It adds a lot more functiobality as opposed to traditional sql
There are 3 types of hive functions (depending upon the type how data is processed):
1.Standard Functions:
It take a row/columns in a row as arguments.
It returns a single row for each row
eg. select concat(frstname," ",lastname) from employee;
other examples pow(),sqrt(),exp(),length,reverse(),regexp_replace()

2.Aggregate Functions:
takes multiple rows as input and return a single result
these functions are generally used with a group by clause
eg. select productname ,sum(revenue) from sales_data group by productname;
other eg.sum(), count(),avg()

3.Table generating functions:
Takes one row as input and outputs multiple rows.
These functions gererally operate on collection data types like array,maps,structs
eg. select explode(array(1,2,3)); //will output 1 row for each array element
---------------------
case..when statement:
computing a column based on conditions.
rule:
employee will get extra vacations 
                                 if tanure <2 ,then 0 days
                                 if tanure 2-3 ,then 2 days
                                 if tanure >2 ,then 3 days
corresponding sql query:
from employee select ename
case when tanure<2 then 0
     when tanure>=2 and tanure<=3 then 2
     when tanure>3 then 3  
end as extra_vacation_days;
------------------
size(): size of elements for array and maps
select size(array(1,2,3)) ;  o/p: 3
select size(map("name","tony","age",30)) ;  o/p: 2

cast(): converts datatype
select cast("25" as bigint); //string to bigint,you can pass column name inplace of "25". it will return null if casting fails
====
RDBMS vs HIVE
suppose we have three rdbms entities:
Employee,Address and Subordinates

Employee:
----------------------------
EmpId  |EmpName  |AddressId
----------------------------
1       tony      1
----------------------------
Address:
------------------------------
AddressId|Street     |City
------------------------------
1         parkstreet  kolkata
------------------------------

Subordinates:
-----------------------------
EmpId   | SubordinateId
----------------------------
1        3
1        4
1        8
---------------------------

We need two joins 2 tables to fetch address by emp name
We need two joins 2 tables to fetch subordinate names for an employee

In RDBMS joins can be made more efficient by introducing indices and constraints(which is not the case for hive)

Hive tries to reduce the number of disk seeks ie. scans of tables as much as possible
In hive we can store the same information in a single table to avoid joins

EmployeeDetails:
-----------------------------------------
EmpId |ename   |Address   |subordinates
-----------------------------------------
1      tony     <STRUCT>    <ARRAY>(eg. jack,jill,bill)
----------------------------------------
This way we can read data with single scan of single table

The data is read in a nested form and then broken apart during the processing phase.
explode() built in function can help in breaking this nested structure( list,maps etc) into individual rows.

eg. select explode(subordinates) from employeeDetails where ename="tony" 
jack
jill
bill

what if we want to include manager name in result?
following will throw error because ename has 1 row but explode(subordinates) has three:
select ename,explode(subordinates) from employeeDetails where ename="tony" 

the result of explode() is actually like a table,which needs to be joined back to original table.This is achieved using "Lateral View"

>select ename,exp.sub from employeeDetails where ename="tony" lateral view explode(subordinates) exp as sub;
O/P
tony jack
tony jill
tony bill

exp => new generated table
sub => a column in newly generated table exp.

lateral view joins the new table with original table,uses inner join by default.
If an emp has null subordinates that emp will be excluded
---------------------
hive>show functions //list of functions
hive> describe function concat; //get documentation for a function
hive> describe function extended concat; //get detailed documentation for a function
hive>!clear;  //use ! to run system command,clear linux command to clean the console

-----------
subqueries:
using one query inside another
we could calculate the union of 2 queries
in hive we can also use subquery to populate a table via insert

hive supports union,union all but does not support following set operations:
minus
intersect


hive supports subqueries only in "form clause" and "where clause"

in "where clause" hive supports subqueries  using "IN/EXISTS"  and "NOT" but not using '=' equality
"IN/NOT IN" subqueries must only select a single column

eg. we have two tables names and revenue
>select names.symbol from names where names.symbol in (select symbol from revenue);
=========
till 28
============
Hive Index:
In RDBMS:
  constraints=> helps in maintaining integrity
  indices => increase query speed

constraints are not supported in hive

index=> index is like a fast lookup from a value to the row numbers in the table that contains that value.

hive allows index creation in two ways:
 1. build-in index
  hive has certain built in methods to create an index
  bitmap index=>used with columns that have very few distinct values. 
                eg. create index emp_index on table employees(ename) as 'BITMAP';
 
  
 2. using an index handler class:
   index handler classes are java classes,hive ships with some of them
   More powerful,anyone can build their own custom index handler,this may be needed in case you want to optimize for a custom scenario.
    eg. create index emp_index on table employees(ename) as 'orh.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';

Deferred Index: You can create an empty index and build the actual index at a later time.
eg. create index emp_index on table employees(ename) as 'BITMAP' with DEFERRED REBUILD; //an index is created but empty

alter index emp_index on employee REBUILD;

 TODO
 
 ===========
 Hive partitions:
  partitions are just splits of all the data in a table.
  each partition is a separate directory.
  each directory can have multiple data file in that partition.
  all partitions together make up the entire table.
  the partition can be done based on the value of one or more column(called partition columns). 

if we decide to make partition based on product column of sales-table,we can have a different partition for each unique product value.
see partition1.png, here we have also removed product column as it is redundant(all entries in same partition has same product) 

see partition2.png to see the directory structure.

why do we partition table?
For  improved performance
Logical organization of data

following only need to scan only one partition instead of scanning whole dataset 
eg.select max(revenue) from sales-table where product='milk'

we have to select partition columns very carefully depending upon the queries we would use in future.
too many partition may optimize certain queries but give bad performance with other queries.
For example consider order table in ecommerce,here partition by customer is not a very good idea.
Thousands of unique customers will result in thousands of partitions.
large No. of partitions= large No. of hadoop directories.
This is a huge overhead for name nodewhich maintains file meta data for hadoop.  

How to define partition in a table:
create table sales_table(
location varchar(30),                 //Note partition column is not specified here
orderDate date,
revenue decimal(10,2)
)
partitioned by (product varchar(30))
 
we can partition by multiple columns:
    
create table sales_table(
location varchar(30),                 
revenue decimal(10,2)
)
partitioned by (
orderDate date,
product varchar(30)) 

it will have directory structure as shown in partition3.png
During query this table just treat it as if it has 4 columns
Queries scanning lesser number of partitions will be faster
---
How do you put stuffs into table with partitions:
create table sales_table
(
location varchar(30),
product varchar(30),
revenue decimal(10,2)
)partitioned by (orderDate date);

insert into sales_table
partition(orderDate='2016-01-16')
values
('kolkata','nutella',7000.50),
('mumbai','milk',5000.50),
('kolkata','milk',6000.50); 


if we have a lots of partitioning it becomes a headache.Then dymanic partitioning is the solution
-----------
Dynamic partitioning:
this creates partitions in a partitioned table automatically
we can load the entire data together and let hive create partitions.
 
configure hive for dynamic partitioning using following commands(this works for current hive session only):
hive>SET hive.exec.dynamic.partition = true;
hive>SET hive.exec.dynamic.partition.mode =nonstrict;

to make dynamic partitioning permanent we need to update hive-site.xml.

there are two modes of dynamic partition:
strict=>there should be atleast one non-dynamic partition
nonstrict=>all partitions are allowed to be dynamic

example:
insert into sales_table
partition(productId,orderDate='2016-01-16')
values
('kolkata',7000.50),
('mumbai',5000.50),
('kolkata',6000.50); 

here productId=>dynamic partitioned as we are not specifying any value
orderDate=>static partition
----
How do you put stuffs into table with dynamic partitions:

lets move records from a non-partitioned table(sales_table_without_partition) to a dynamic partitioned table(sales_table)

create table sales_table_without_partition
(
location varchar(30),
product varchar(30),
revenue decimal(10,2),
orderDate date)
);


create table sales_table
(
location varchar(30),
revenue decimal(10,2)
)partitioned by (product varchar(30),orderDate date);

insert into sales_data
partition(product,orderDate)
select location,revenue,product,orderDate //it is necessary to include the partition columns as the last columns(product,orderDate) in the query
from sales_table_without_partition;

It will result in directory structure as shown in partition4.png
------------
how to check what partitions exists in a table?
>show partitions <table name>;
we can also check inside hdfs
=================
Bucketing:
Like partitioning it is another technique to split the data to gain better query performance.

analogy:
partitioning=>b-tree index
bucket=>hashing

buckets can apply to both partitioned and non-partitioned tables.

too many partitions may overwelm namenode,infact hive restricts the max number of partitions to avoid this very scenario.
Lets say you have a table with userIds,if you create partition based on userId, hive will not allow if it crosses max partition threshold.

Bucketing  breaks the data into buckets(using hash function) so that we always know which bucket a user belongs to.
Now to find the existance of a user we just need to look into one bucket.
All rows with same value for bucketing cloumn will end up in the same bucket, we can use this feature for faster lookup.
bucketing storage directory structure is shown in partition5.png

Bucketing  allows rows in each bucket to be sorted by one or more columns,as a result join of each bucket is an efficient merge sort.
advantage over partitioning:
No. of buckets is fixed and not dependent on the possible values of column.

eg.
create table reviews(
movieId int,
reviewId bigint;
review varchar(200),
rating tinyint
)
clustered by (reviewId) into 4 buckets; //we are specifying 4 buckets on reviewId column 

---------
partitioning + bucketing

create table sales(
productId int,
orderDate date,
revenue decimal(10,2)
)partitioned by (storeId int)
clustered by (productId) into 4 buckets;

Its directory structure is shown in partition6.png
-----------
creating a table with buckets that have sorted records:
eg.
create table sales(
productId int,
orderDate date,
revenue decimal(10,2)
)partitioned by (storeId int)
clustered by (productId)
sorted by (orderDate) into 4 buckets; //records inside the bucket is sorted by orderDate
------------
to populate the bucketed table we have to set following property:
hive>SET hive.enforce.bucketing = true; //No need to set this from hive 2.X



 











  
